/**
 * åƒé—® VLM + Tesseract.js æ··åˆ OCR Demo
 *
 * æµç¨‹ï¼š
 * 1. VLM è¯†åˆ«ä¸»æ–‡å­—åŒºåŸŸè¾¹ç•Œæ¡†
 * 2. Sharp è£å‰ªè¯¥åŒºåŸŸ
 * 3. Tesseract.js å¯¹è£å‰ªåŒºåŸŸè¿›è¡Œ OCR
 */

import OpenAI from "openai";
import { createWorker } from "tesseract.js";
import sharp from "sharp";
import * as fs from "fs";
import * as path from "path";

const openai = new OpenAI({
  apiKey: "sk-eda437625d1c4c09a9b58cc567b9ddcc",
  baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1",
});

interface TextRegion {
  box: number[]; // [left, top, width, height]
  confidence: number;
}

function imageToBase64(imagePath: string): string {
  const absolutePath = path.resolve(imagePath);
  const imageBuffer = fs.readFileSync(absolutePath);
  const ext = path.extname(imagePath).toLowerCase().slice(1);
  const mimeType = ext === "jpg" ? "jpeg" : ext;
  return `data:image/${mimeType};base64,${imageBuffer.toString("base64")}`;
}

/**
 * Step 1: ä½¿ç”¨ VLM è¯†åˆ«ä¸»æ–‡å­—åŒºåŸŸ
 */
async function detectTextRegion(imagePath: string): Promise<TextRegion | null> {
  console.log("\nğŸ“ Step 1: VLM è¯†åˆ«ä¸»æ–‡å­—åŒºåŸŸ...");
  const startTime = performance.now();

  const imageBase64 = imageToBase64(imagePath);
  console.log(`   Image size: ${(imageBase64.length / 1024).toFixed(1)}KB`);

  const prompt = `åˆ†æè¿™å¼ æˆªå›¾ï¼Œè¯†åˆ«å‡ºæœ€æ ¸å¿ƒï¼Œæœ€æ¸…æ™°çš„æ–‡å­—å†…å®¹åŒºåŸŸï¼ˆå³æ­£æ–‡éƒ¨åˆ†ï¼Œæ’é™¤å¯¼èˆªæ ã€ä¾§è¾¹æ ã€å¹¿å‘Šç­‰ï¼‰ã€‚
è¯·åŠ¡å¿…è¿”å›è¯¥åŒºåŸŸåœ¨å›¾ç‰‡ä¸­çš„åƒç´ åæ ‡ï¼Œæ ¼å¼ä¸º [left, top, width, height]ã€‚

è¿”å› JSON æ ¼å¼ï¼š
{
  "content_type": "document|blog|code|other",
  "text_region": {
    "box": [left, top, width, height],
    "confidence": 0.95
  }
}

æ³¨æ„ï¼š
1. box å­—æ®µå¿…é¡»æ˜¯ [å·¦è¾¹ç•Œ, ä¸Šè¾¹ç•Œ, å®½åº¦, é«˜åº¦] çš„æ•°å€¼æ•°ç»„ã€‚
2. è¯·ç¡®ä¿åæ ‡åœ¨å›¾ç‰‡èŒƒå›´å†…ã€‚`;

  const response = await openai.chat.completions.create({
    model: "qwen3-vl-plus",
    messages: [
      {
        role: "user",
        content: [
          { type: "image_url", image_url: { url: imageBase64 } },
          { type: "text", text: prompt },
        ],
      },
    ],
  });

  const vlmTime = performance.now() - startTime;
  console.log(`   â±ï¸ VLM time: ${(vlmTime / 1000).toFixed(2)}s`);

  const content = response.choices[0]?.message?.content || "";
  console.log(`   ğŸ“„ VLM Response:\n${content}`);

  // è§£æ JSON å“åº”
  try {
    const jsonMatch = content.match(/```json\s*([\s\S]*?)```/) || content.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      const jsonStr = jsonMatch[1] || jsonMatch[0];
      const parsed = JSON.parse(jsonStr);
      return parsed.text_region as TextRegion;
    }
  } catch (e) {
    console.log("   âš ï¸ Failed to parse JSON response", e);
  }
  return null;
}

/**
 * Step 2: è£å‰ªå›¾ç‰‡åŒºåŸŸ
 */
async function cropRegion(imagePath: string, box: number[]): Promise<Buffer> {
  console.log("\nâœ‚ï¸ Step 2: è£å‰ªä¸»æ–‡å­—åŒºåŸŸ...");

  // è·å–å›¾ç‰‡å®é™…å°ºå¯¸
  const metadata = await sharp(imagePath).metadata();
  const imgWidth = metadata.width || 1920;
  const imgHeight = metadata.height || 1080;
  console.log(`   Image size: ${imgWidth}x${imgHeight}`);

  let [left, top, width, height] = box;

  // è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿ä¸è¶…è¿‡å›¾ç‰‡å°ºå¯¸
  left = Math.max(0, Math.min(left, imgWidth - 1));
  top = Math.max(0, Math.min(top, imgHeight - 1));
  width = Math.min(width, imgWidth - left);
  height = Math.min(height, imgHeight - top);

  console.log(`   Region: left=${left}, top=${top}, width=${width}, height=${height}`);

  const croppedBuffer = await sharp(imagePath)
    .extract({
      left: Math.round(left),
      top: Math.round(top),
      width: Math.round(width),
      height: Math.round(height),
    })
    .toBuffer();

  console.log(`   âœ… Cropped size: ${(croppedBuffer.length / 1024).toFixed(1)}KB`);
  return croppedBuffer;
}

/**
 * Step 3: Tesseract OCR
 */
async function performOCR(
  imageBuffer: Buffer,
  lang: string = "eng+chi_sim"
): Promise<{ text: string; confidence: number }> {
  console.log(`\nğŸ” Step 3: Tesseract OCR (${lang})...`);
  const startTime = performance.now();

  const worker = await createWorker(lang, 1, {
    logger: (m) => {
      if (m.status === "recognizing text") {
        process.stdout.write(`\r   ğŸ” Recognizing: ${(m.progress * 100).toFixed(0)}%`);
      }
    },
  });

  try {
    const {
      data: { text, confidence },
    } = await worker.recognize(imageBuffer);
    const ocrTime = performance.now() - startTime;

    console.log(`   â±ï¸ OCR time: ${(ocrTime / 1000).toFixed(2)}s`);
    console.log(`   ğŸ¯ Confidence: ${confidence.toFixed(1)}%`);

    return { text: text.trim(), confidence };
  } finally {
    await worker.terminate();
  }
}

async function main() {
  const imagePath = process.argv[2];

  if (!imagePath) {
    console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     åƒé—® VLM + Tesseract.js æ··åˆ OCR Demo                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Usage:
  npx tsx demo/qwen-vlm-demo.ts <image_path> [--chi]

Options:
  --chi    Use Chinese OCR (chi_sim)
`);
    return;
  }

  const totalStart = performance.now();
  console.log("\n" + "=".repeat(60));
  console.log("ğŸ§  VLM + Tesseract.js æ··åˆ OCR Demo");
  console.log("=".repeat(60));
  console.log(`ğŸ“ Image: ${path.basename(imagePath)}`);

  try {
    // Step 1: VLM è¯†åˆ«åŒºåŸŸ
    const region = await detectTextRegion(imagePath);

    if (!region || !region.box) {
      console.log("\nâŒ Failed to detect text region");
      return;
    }

    // Step 2: è£å‰ªåŒºåŸŸ
    const croppedBuffer = await cropRegion(imagePath, region.box);

    // Step 3: Tesseract OCR
    const result = await performOCR(croppedBuffer);

    // è¾“å‡ºç»“æœ
    const totalTime = performance.now() - totalStart;
    console.log("\n" + "=".repeat(60));
    console.log("ğŸ“Š Final Results");
    console.log("=".repeat(60));
    console.log(`â±ï¸  Total time: ${(totalTime / 1000).toFixed(2)}s`);
    console.log(`ğŸ¯ OCR Confidence: ${result.confidence.toFixed(1)}%`);
    console.log(`\nğŸ“„ Recognized Text:`);
    console.log("â”€".repeat(40));
    console.log(result.text || "[No text detected]");
  } catch (error) {
    console.error("\nâŒ Error:", error);
  }

  console.log("\n" + "=".repeat(60) + "\n");
}

main();
